{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-24T01:24:22.025615Z","iopub.execute_input":"2024-07-24T01:24:22.026260Z","iopub.status.idle":"2024-07-24T01:24:23.011347Z","shell.execute_reply.started":"2024-07-24T01:24:22.026227Z","shell.execute_reply":"2024-07-24T01:24:23.010377Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"huggi_fac\")\nsecret_value_1 = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:24:34.945651Z","iopub.execute_input":"2024-07-24T01:24:34.946034Z","iopub.status.idle":"2024-07-24T01:24:35.344810Z","shell.execute_reply.started":"2024-07-24T01:24:34.946003Z","shell.execute_reply":"2024-07-24T01:24:35.344072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"%%capture is a magic command in Jupyter notebook that suppresses the output of the cell.\n\n‚Ä¢ %pip install -U transformers installs or upgrades the 'transformers' library, which is used for NLP tasks.\n\n‚Ä¢ %pip install -U datasets installs or upgrades the 'datasets' library, used for managing datasets.\n\n‚Ä¢ %pip install -U accelerate installs or upgrades the 'accelerate' library, used for distributed training.\n\n‚Ä¢ %pip install -U peft installs or upgrades the 'peft' library, which is a performance evaluation tool.\n\n‚Ä¢ %pip install -U trl installs or upgrades the 'trl' library, used for reinforcement learning tasks.\n\n‚Ä¢ %pip install -U bitsandbytes installs or upgrades the 'bitsandbytes' library, used for fast PyTorch operations.\n\n‚Ä¢ %pip install -U wandb installs or upgrades the 'wandb' library, used for experiment tracking.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:25:07.088180Z","iopub.execute_input":"2024-07-24T01:25:07.088533Z","iopub.status.idle":"2024-07-24T01:27:05.185229Z","shell.execute_reply.started":"2024-07-24T01:25:07.088505Z","shell.execute_reply":"2024-07-24T01:27:05.184024Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The code begins by importing various modules from the transformers library.\n\n‚Ä¢ AutoModelForCausalLM is used for causal language modeling.\n\n‚Ä¢ AutoTokenizer is used to tokenize text data.\n\n‚Ä¢ BitsAndBytesConfig is a configuration class for the Bits and Bytes model.\n\n‚Ä¢ HfArgumentParser is a parser for command line arguments.\n\n‚Ä¢ TrainingArguments is used to define training parameters.\n\n‚Ä¢ pipeline is a high-level, easy to use, API for doing machine learning tasks. \n\n‚Ä¢ logging is used for logging purposes.\n\n‚Ä¢ It then imports various modules from the peft library.\n\n‚Ä¢ LoraConfig is a configuration class for the Lora model.\n\n‚Ä¢ PeftModel is the main model class.\n\n‚Ä¢ prepare_model_for_kbit_training is a function to prepare the model for training.\n\n‚Ä¢ get_peft_model is a function to get the PEFT model.\n\n‚Ä¢ It imports the os module for interacting with the operating system.\n\n‚Ä¢ torch is imported for tensor computations and neural networks.\n\n‚Ä¢ wandb is imported for experiment tracking and visualization.\n\n‚Ä¢ load_dataset is imported from datasets to load datasets.\n\n‚Ä¢ SFTTrainer and setup_chat_format are imported from trl for training and setting up chat format.","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:28:03.515095Z","iopub.execute_input":"2024-07-24T01:28:03.516097Z","iopub.status.idle":"2024-07-24T01:28:22.367188Z","shell.execute_reply.started":"2024-07-24T01:28:03.516061Z","shell.execute_reply":"2024-07-24T01:28:22.366376Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-07-24 01:28:10.233779: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-24 01:28:10.233912: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-24 01:28:10.356246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"huggi_fac\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:34:17.806200Z","iopub.execute_input":"2024-07-24T01:34:17.807541Z","iopub.status.idle":"2024-07-24T01:34:37.956052Z","shell.execute_reply.started":"2024-07-24T01:34:17.807504Z","shell.execute_reply":"2024-07-24T01:34:37.954843Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mds4maths\u001b[0m (\u001b[33mds4maths-indian-institute-of-technology-madras\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240724_013420-7d4ryncc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/7d4ryncc' target=\"_blank\">solar-star-1</a></strong> to <a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/7d4ryncc' target=\"_blank\">https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/7d4ryncc</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:36:27.121532Z","iopub.execute_input":"2024-07-24T01:36:27.121938Z","iopub.status.idle":"2024-07-24T01:36:27.127351Z","shell.execute_reply.started":"2024-07-24T01:36:27.121908Z","shell.execute_reply":"2024-07-24T01:36:27.126324Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The first line of code is defining a variable torch_dtype and assigning it the value torch.float16.\n\n‚Ä¢ torch.float16 is a data type in PyTorch, a machine learning library in Python.\n\n‚Ä¢ This data type is used for storing tensor elements in half precision floating point format.\n\n‚Ä¢ The second line of code is defining a variable attn_implementation and assigning it the string \"eager\".\n\n‚Ä¢ This could be a setting for how attention mechanisms are implemented in a neural network model.","metadata":{}},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:36:52.775334Z","iopub.execute_input":"2024-07-24T01:36:52.775692Z","iopub.status.idle":"2024-07-24T01:36:52.781446Z","shell.execute_reply.started":"2024-07-24T01:36:52.775662Z","shell.execute_reply":"2024-07-24T01:36:52.780469Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The code begins by setting up a configuration for QLoRA (Quantized Long Range Arena) using BitsAndBytesConfig.\n\n‚Ä¢ load_in_4bit=True indicates that the model weights will be loaded in 4-bit precision.\n\n‚Ä¢ bnb_4bit_quant_type=\"nf4\" sets the type of 4-bit quantization to be used.\n\n‚Ä¢ bnb_4bit_compute_dtype=torch_dtype sets the data type for computations to be the same as torch_dtype.\n\n‚Ä¢ bnb_4bit_use_double_quant=True enables the use of double quantization.\n\n‚Ä¢ The AutoModelForCausalLM.from_pretrained method is then used to load a pre-trained model for causal language modeling.\n\n‚Ä¢ The base_model parameter specifies the model to be loaded.\n\n‚Ä¢ quantization_config=bnb_config applies the previously defined quantization configuration to the model.\n\n‚Ä¢ device_map=\"auto\" allows the model to be automatically mapped to the available device (CPU or GPU).\n\n‚Ä¢ attn_implementation=attn_implementation sets the type of attention mechanism to be used in the model.","metadata":{}},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:38:32.991930Z","iopub.execute_input":"2024-07-24T01:38:32.992256Z","iopub.status.idle":"2024-07-24T01:40:17.237859Z","shell.execute_reply.started":"2024-07-24T01:38:32.992233Z","shell.execute_reply":"2024-07-24T01:40:17.236756Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96a7bf7a63e405a8809023a2af5674c"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:43:26.133434Z","iopub.execute_input":"2024-07-24T01:43:26.134164Z","iopub.status.idle":"2024-07-24T01:43:26.681136Z","shell.execute_reply.started":"2024-07-24T01:43:26.134131Z","shell.execute_reply":"2024-07-24T01:43:26.680025Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Adding the adapter to the layer\n\nFine-tuning the full model will take a lot of time, so to improve the training time, we‚Äôll attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient.","metadata":{}},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:44:02.726880Z","iopub.execute_input":"2024-07-24T01:44:02.727577Z","iopub.status.idle":"2024-07-24T01:44:03.760331Z","shell.execute_reply.started":"2024-07-24T01:44:02.727545Z","shell.execute_reply":"2024-07-24T01:44:03.759340Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The dataset is imported using the load_dataset function, specifying the dataset name and split type.\n\n‚Ä¢ The dataset is shuffled with a seed of 65 and then the first 1000 samples are selected.\n\n‚Ä¢ A function format_chat_template is defined to format the data into a chat template.\n\n‚Ä¢ This function takes a row of data and creates a JSON object with the roles of \"user\" and \"assistant\".\n\n‚Ä¢ The content of the \"user\" and \"assistant\" roles are taken from the \"Patient\" and \"Doctor\" fields of the row.\n\n‚Ä¢ The chat template is then applied to the row using the apply_chat_template method of the tokenizer.\n\n‚Ä¢ The format_chat_template function is then applied to the entire dataset using the map function.\n\n‚Ä¢ The num_proc parameter is set to 4, which means that the mapping operation will be done in parallel using 4 processes.\n\n‚Ä¢ Finally, the fourth element of the 'text' column in the dataset is displayed.","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:47:51.048777Z","iopub.execute_input":"2024-07-24T01:47:51.049196Z","iopub.status.idle":"2024-07-24T01:47:56.783626Z","shell.execute_reply.started":"2024-07-24T01:47:51.049165Z","shell.execute_reply":"2024-07-24T01:47:56.782380Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75980171a0dc47ecb6e5a43566945dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d967ce0099de467d89bb6383997a960a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92eed4319c5a41edaef72750af0df1d1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce614194e78c4bc89623f1bfc0ee8938"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|im_end|>\\n<|im_start|>assistant\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:53:12.706262Z","iopub.execute_input":"2024-07-24T01:53:12.706685Z","iopub.status.idle":"2024-07-24T01:53:12.725158Z","shell.execute_reply.started":"2024-07-24T01:53:12.706652Z","shell.execute_reply":"2024-07-24T01:53:12.724245Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The code is creating an instance of the TrainingArguments class with specific parameters.\n\n‚Ä¢ output_dir=new_model sets the directory where the model and its metadata will be saved.\n\n‚Ä¢ per_device_train_batch_size=1 and per_device_eval_batch_size=1 set the batch size for training and evaluation.\n\n‚Ä¢ gradient_accumulation_steps=2 specifies the number of steps to accumulate gradients before updating parameters.\n\n‚Ä¢ optim=\"paged_adamw_32bit\" sets the optimization algorithm to be used during training.\n\n‚Ä¢ num_train_epochs=1 sets the number of times the training loop will iterate over all training data.\n\n‚Ä¢ evaluation_strategy=\"steps\" and eval_steps=0.2 define the evaluation to be done every 0.2 steps.\n\n‚Ä¢ logging_steps=1 and logging_strategy=\"steps\" set the logging to be done every step.\n\n‚Ä¢ warmup_steps=10 sets the number of steps for the warm-up phase, where the learning rate is gradually increased.\n\n‚Ä¢ learning_rate=2e-4 sets the initial learning rate for the optimizer.\n\n‚Ä¢ fp16=False and bf16=False disable the use of 16-bit floating point precision during training.\n\n‚Ä¢ group_by_length=True enables the grouping of samples of similar length into the same batch.\n\n‚Ä¢ report_to=\"wandb\" sets the reporting destination to Weights & Biases, a tool for tracking and visualizing machine learning experiments.","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T01:54:02.503514Z","iopub.execute_input":"2024-07-24T01:54:02.503872Z","iopub.status.idle":"2024-07-24T01:54:02.542045Z","shell.execute_reply.started":"2024-07-24T01:54:02.503844Z","shell.execute_reply":"2024-07-24T01:54:02.540963Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code is initializing a trainer object from the SFTTrainer class.\n\n‚Ä¢ The model argument is the machine learning model that will be trained.\n\n‚Ä¢ train_dataset and eval_dataset are the training and testing datasets respectively.\n\n‚Ä¢ peft_config is the configuration for the Pretraining with Extracted Feature Tokens (PEFT).\n\n‚Ä¢ max_seq_length sets the maximum length of the sequences that the model will handle.\n\n‚Ä¢ dataset_text_field specifies the field in the dataset that contains the text to be processed.\n\n‚Ä¢ tokenizer is the tokenizer that will be used to process the text.\n\n‚Ä¢ args are the arguments for the training process.\n\n‚Ä¢ packing is a boolean that determines whether to pack multiple sequences together for efficiency.","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:10:29.077981Z","iopub.execute_input":"2024-07-24T02:10:29.078334Z","iopub.status.idle":"2024-07-24T02:10:30.319653Z","shell.execute_reply.started":"2024-07-24T02:10:29.078307Z","shell.execute_reply":"2024-07-24T02:10:30.318528Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53a3e66b7f741d4814ee8802bf2c60b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6a0b210c834e1581a428a272c2c119"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:11:56.606814Z","iopub.execute_input":"2024-07-24T02:11:56.607691Z","iopub.status.idle":"2024-07-24T02:41:24.343655Z","shell.execute_reply.started":"2024-07-24T02:11:56.607659Z","shell.execute_reply":"2024-07-24T02:41:24.342755Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 29:20, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>1.921100</td>\n      <td>2.509598</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.394600</td>\n      <td>2.459979</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.346900</td>\n      <td>2.428414</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.455200</td>\n      <td>2.406353</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.489200</td>\n      <td>2.389822</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=2.5913664221763613, metrics={'train_runtime': 1766.7287, 'train_samples_per_second': 0.509, 'train_steps_per_second': 0.255, 'total_flos': 9265767453646848.0, 'train_loss': 2.5913664221763613, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:43:53.572173Z","iopub.execute_input":"2024-07-24T02:43:53.572555Z","iopub.status.idle":"2024-07-24T02:43:56.691649Z","shell.execute_reply.started":"2024-07-24T02:43:53.572525Z","shell.execute_reply":"2024-07-24T02:43:56.690857Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.036 MB of 0.036 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñà‚ñÖ‚ñÅ‚ñÉ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÑ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.38982</td></tr><tr><td>eval/runtime</td><td>75.6422</td></tr><tr><td>eval/samples_per_second</td><td>1.322</td></tr><tr><td>eval/steps_per_second</td><td>1.322</td></tr><tr><td>total_flos</td><td>9265767453646848.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>3.13137</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4892</td></tr><tr><td>train_loss</td><td>2.59137</td></tr><tr><td>train_runtime</td><td>1766.7287</td></tr><tr><td>train_samples_per_second</td><td>0.509</td></tr><tr><td>train_steps_per_second</td><td>0.255</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">solar-star-1</strong> at: <a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/7d4ryncc' target=\"_blank\">https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/7d4ryncc</a><br/> View project at: <a href='https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/ds4maths-indian-institute-of-technology-madras/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240724_013420-7d4ryncc/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:47:34.256527Z","iopub.execute_input":"2024-07-24T02:47:34.257351Z","iopub.status.idle":"2024-07-24T02:47:51.166529Z","shell.execute_reply.started":"2024-07-24T02:47:34.257315Z","shell.execute_reply":"2024-07-24T02:47:51.165608Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\nHello. I have gone through your query. I can understand your concern. Acne is a common problem. It can be treated with the help of medicines and lifestyle changes. I would suggest you to use a topical retinoid cream like Adapalene or Tretinoin. You can also use a topical antibiotic like Clindamycin or Erythromycin. You can also take oral antibiotics like Doxycycline or Minocycline. You can also take oral retinoids like Isotretinoin. You can also use a face wash containing salicylic acid. You can also\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T02:48:04.231499Z","iopub.execute_input":"2024-07-24T02:48:04.232223Z","iopub.status.idle":"2024-07-24T02:48:15.580034Z","shell.execute_reply.started":"2024-07-24T02:48:04.232192Z","shell.execute_reply":"2024-07-24T02:48:15.578961Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beee99e8d96f4c7f83f89f06dc091894"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Deepakiitm/llama-3-8b-chat-doctor/commit/0abe0a329d73de79273a9b8d51d058ad662bd6d8', commit_message='Upload model', commit_description='', oid='0abe0a329d73de79273a9b8d51d058ad662bd6d8', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}